import base64
import io
import time
import os
import numpy as np
import requests
import cv2
import datetime
from typing import Tuple, Union, Optional
import argparse
import collections
import json_numpy
from datasets.common import quat_to_rotate6d, rotate6d_to_quat
# ä»æ–°æœºå™¨äººSDKå¯¼å…¥
from a2d_sdk.robot import RobotDds, RobotController, CosineCamera
# --- å…¨å±€é…ç½® ---
LOG_IMAGE_DIR = "./Log"  # ä¿å­˜å‘¨æœŸæ€§å›¾åƒæ—¥å¿—çš„æ–‡ä»¶å¤¹

CAMERA_MAPPING = {
    "cam_head": "head",
    "cam_left_wrist": "hand_left",
    "cam_right_wrist": "hand_right",
}

TASK_INFOS = {
    0: {
        'instruction': "Pick up the object and place it in the bag.",
        'arm_init': [
                    -1.11224556,  0.53719825,  0.45914441, -1.23825192,  0.5959, 1.41219366, -0.08660435,
                    1.07460594, -0.61097687, -0.2804215, 1.28363943, -0.72993356, -1.4951334, 0.18722105,
                    ],
        'gripper_init': [
                    0.0,
                    0.0
                    ],
        'waist_init':  [
                    0.52359929,
                    27,
                    ],
        'head_init': [
                    0.0,
                    0.436332306,],
    },
    1: {
        'instruction': "Pick objects from the conveyor belt and place them in the box.",
        'arm_init': [
                    -1.085, 0.5951, 0.3214, -1.279, 0.7025, 1.479, -0.1656,
                    1.075, -0.6117, -0.2797, 1.282, -0.7310, -1.495, 0.1868,
                    ],
        'gripper_init': [
                    0.0,
                    0.0
                    ],
        'waist_init': [
                    0.5236,
                    26.0,
                    ],
        'head_init': [
                    0.0,
                    0.4363,
                    ],
    },
    2: {
        'instruction': "Hang the snacks on the shelf.",
        'arm_init': [
                    -1.686, 0.9457, 1.330, -0.8735, 0.1478, 1.252, 0.03354,
                    1.815, -0.6138, -1.393, 0.8388, -0.1517, -1.466, -0.06992,
                    ],# å’Œæ•°é‡‡ä½ç½®å¯¹é½
        'gripper_init': [0.0, 0.0],
        'waist_init': [
                    0.1920, 
                    31
                    ],
        'head_init': [
                    0.0,
                    0.4363,
                    ],
    },
    3: {
        'instruction': "pour the water into the cup.",
        'arm_init': [
                    -1.03729784, 0.58743685, 0.27705365, -1.23694324, 0.70110613, 1.44067192, -0.17989205,
                    1.07420456, -0.611099, -0.27960137, 1.28388369, -0.73043954, -1.49543011, 0.1876224,
                    ], # å’Œæ•°é‡‡ä½ç½®å¯¹é½
        'gripper_init': [0.0, 0.0],
        'waist_init': [
                    0.52359771, 
                    29.999931
                    ],
        'head_init': [
                    0.0,
                    0.43633231,
                    ],
    },
    4: {
        'instruction': "Open the microwave, put the food in, and close the microwave.",
        'arm_init': [
                    -1.0742743, 0.61099428, 0.279549, -1.28383136, 0.73043954, 1.49532545, -0.1876224,
                    1.07420456, -0.61097687, -0.2795839, 1.28395355, -0.73038721, -1.49534285, 0.18760496,
                    ],# å’Œæ•°é‡‡ä½ç½®å¯¹é½
        'gripper_init': [
                    0.0,
                    0.0
                    ],
        'waist_init': [
                    0.43633204,
                    24.0
                    ],
        'head_init': [
                    0.0,
                    0.43633231,
                    ],
    },
    5: {
        'instruction': "fold the clothes",
        'arm_init': [
                    -1.0742219686508179, 0.6111513376235962, 0.27946174144744873, -1.2839535474777222, 0.7303872108459473, 1.495360255241394, -0.18760496377944946,
                    1.0742743015289307, -0.6110466122627258, -0.2794792056083679, 1.2838836908340454, -0.7303697466850281, -1.4952380657196045, 0.18762239813804626,
                    ], # å’Œæ•°é‡‡ä½ç½®å¯¹é½
        'gripper_init': [0.0, 0.0],
        'waist_init': [
                    0.8901176920412174, 
                    45.98676300048828
                    ],
        'head_init': [
                    0.0,
                    0.4363323055555555,
                    ],
    }
}

def to_flat_array(x, dtype=np.float32):
    return np.asarray(x, dtype=dtype).ravel()

class ClientModel():
    def __init__(self,
                 host,
                 port,
                 chunk_size=20,
                 control_mode='abs_eef',
                 close_loop=False,
                 domain_id=20):

        self.url = f"http://{host}:{port}/act"
        self.reset()
        self.chunk_size = chunk_size
        self.control_mode = control_mode
        self.close_loop = close_loop
        self.domain_id = domain_id
        
    def reset(self):
        self.proprio = None
        self.action_plan = collections.deque()
        return None

    def step(self, obs, proprio, instruction):
           
        print(self.domain_id, instruction)
        if not self.action_plan:
            print(obs['cam_head'].shape)
            if self.proprio is None: self.proprio = to_flat_array(proprio)
            if self.close_loop: self.proprio = to_flat_array(proprio) 
            query = {
                "proprio": json_numpy.dumps(self.proprio),
                "language_instruction": instruction,
                "image0": json_numpy.dumps(obs['cam_head']),
                "image1": json_numpy.dumps(obs['cam_left_wrist']),
                "image2": json_numpy.dumps(obs['cam_right_wrist']),
                "domain_id": self.domain_id
            }
            response = requests.post(self.url, json=query)
            actions = np.array(response.json()['action'])[:self.chunk_size]
            actions = self.post_process(actions)
            self.action_plan.extend(actions.tolist())
        
        action_predict = np.array(self.action_plan.popleft())
        if not self.close_loop: 
            if 'ee' in self.control_mode:
                self.proprio = np.concatenate([
                    action_predict[:3],
                    quat_to_rotate6d(action_predict[3:7]),
                    action_predict[7:8],
                    action_predict[8:11],
                    quat_to_rotate6d(action_predict[11:15]),
                    action_predict[15:16]], axis=-1)
            else: self.proprio = action_predict
        # action_predict = self.post_process(action_predict)
        return action_predict
    
    def post_process(self, action):
        # proprio: the env proprioception
        # action: the model vanilla output 
        
        if self.control_mode == "abs_joint":
            return action
        elif self.control_mode == "delta_joint":
            left_joint = action[:, 0:7] + self.proprio[None, 0:7]
            right_joint = action[:, 7:14] + self.proprio[None, 7:14]
            left_gripper = action[:, 14:15]
            right_gripper = action[:, 15:16]
            return np.concatenate([left_joint, right_joint, left_gripper, right_gripper], axis=-1)
        elif self.control_mode == "abs_eef":
            right_xyz = action[:, 0:3]
            right_quat = rotate6d_to_quat(action[:, 3:9])
            right_gripper = action[:, 9:10]
            left_xyz = action[:, 10:13]
            left_quat = rotate6d_to_quat(action[:, 13:19])
            left_gripper = action[:, 19:20]
            return np.concatenate([left_xyz, left_quat, left_gripper, right_xyz, right_quat, right_gripper], axis=-1)
        elif self.control_mode == "delta_eef":
            left_xyz = action[:, 0:3] + self.proprio[None, 0:3]
            left_quat = rotate6d_to_quat(action[:, 3:9], scalar_first=True)
            left_gripper = action[:, 9:10]
            
            right_xyz = action[:, 10:13] + self.proprio[None, 10:13]
            right_quat = rotate6d_to_quat(action[:, 13:19], scalar_first=True)
            right_gripper = action[:, 19:20]

            return np.concatenate([left_xyz, left_quat, left_gripper, right_xyz, right_quat, right_gripper], axis=-1)
            
            
def encode_image(img: np.ndarray) -> str:
    """å°† OpenCV å›¾åƒç¼–ç ä¸º base64 PNG å­—ç¬¦ä¸²ã€‚"""
    _, buffer = cv2.imencode('.png', img)
    return base64.b64encode(buffer).decode('utf-8')

def clear_log_directory():
    """æ¸…ç©º LOG_IMAGE_DIR æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    if os.path.exists(LOG_IMAGE_DIR):
        for filename in os.listdir(LOG_IMAGE_DIR):
            file_path = os.path.join(LOG_IMAGE_DIR, filename)
            try:
                if os.path.isfile(file_path):
                    os.remove(file_path)
                    print(f"å·²åˆ é™¤æ–‡ä»¶: {file_path}")
            except Exception as e:
                print(f"è­¦å‘Šï¼šæ— æ³•åˆ é™¤æ–‡ä»¶ {file_path}: {e}")
    else:
        os.makedirs(LOG_IMAGE_DIR)
        print(f"ğŸ“‚ å·²åˆ›å»ºæ–‡ä»¶å¤¹: {LOG_IMAGE_DIR}")

def get_and_encode_image(camera: CosineCamera, cam_sdk_name: str) -> Tuple[np.ndarray, Optional[str]]:
    """
    ä»æŒ‡å®šçš„æ‘„åƒå¤´è·å–å›¾åƒï¼Œè¿”å›åŸå§‹å›¾åƒå’Œç¼–ç åçš„å­—ç¬¦ä¸²ã€‚
    """
    try:
        img, _ = camera.get_latest_image(cam_sdk_name)
        if img is not None and img.size > 0:
            # encoded_str = encode_image(img)
            return img, None
        else:
            print(f"è­¦å‘Šï¼šæ— æ³•è·å– {cam_sdk_name} çš„å›¾åƒï¼Œæˆ–è€…å›¾åƒä¸ºç©ºã€‚")
            return None, None
    except Exception as e:
        print(f"è­¦å‘Šï¼šè·å– {cam_sdk_name} å›¾åƒæ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
        return None, None

def main(args):
    """ä¸»ç¨‹åºï¼šè¿æ¥æœºå™¨äººï¼Œå¹¶è¿›å…¥ä¸»æ§åˆ¶å¾ªç¯"""
    if not os.path.exists(LOG_IMAGE_DIR):
        os.makedirs(LOG_IMAGE_DIR)
        print(f"ğŸ“‚ Directory has been made: {LOG_IMAGE_DIR}")
        # æ¸…ç©º Log æ–‡ä»¶å¤¹
    print("ğŸ§¹ Sweep logs...")
    clear_log_directory()

    robot_dds = None
    robot_controller = None
    camera = None
    domain_id = int(args.task_id) + 20  # AGIBOT domain_id is 15
    agent = ClientModel(args.server_ip, args.server_port, args.chunk_size, args.control_mode, args.close_loop, domain_id=domain_id)
    interval_time = 1.0 / args.control_freq

    try:
        # --- 1. åˆå§‹åŒ–æœºå™¨äººå’Œç›¸æœº ---
        print("ğŸ¤– Init robot...")
        robot_dds = RobotDds()
        robot_controller = RobotController()
        
        # ä» CAMERA_MAPPING è·å–æ‰€æœ‰éœ€è¦ä½¿ç”¨çš„æ‘„åƒå¤´SDKåç§°
        camera_sdk_names = list(CAMERA_MAPPING.values())
        print(f"ğŸ“· Init cameras: {camera_sdk_names}")
        camera = CosineCamera(camera_sdk_names)
        
        robot_dds.reset(arm_positions=TASK_INFOS[args.task_id]['arm_init'],
                        gripper_positions=TASK_INFOS[args.task_id]['gripper_init'],
                        hand_positions=robot_dds.hand_initial_joint_position,
                        waist_positions=TASK_INFOS[args.task_id]['waist_init'],
                        head_positions=TASK_INFOS[args.task_id]['head_init'])
        current_instruction = TASK_INFOS[args.task_id]['instruction']
        print(f"ğŸ“ Current task_id: {args.task_id}, instruction: {current_instruction}")
        print("âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼")
        # input()
        print("ğŸš€ è¿›å…¥ä¸»æ§åˆ¶å¾ªç¯...")

        # --- 2. ä¸»æ§åˆ¶å¾ªç¯ ---
        count = 0
        while True:
            time.sleep(interval_time)
            print("\n" + "="*50)
            
            # --- 2.1. è·å–çŠ¶æ€å’Œå›¾åƒ ---
            try:

                motion_status = robot_controller.get_motion_status()
                left_cartesian = motion_status["frames"]["arm_left_link7"]
                right_cartesian = motion_status["frames"]["arm_right_link7"]
                #a2d_sdk.gripper_states() è¿”å› ([å·¦çˆªçŠ¶æ€, å³çˆªçŠ¶æ€], [æ—¶é—´æˆ³]) å¾…ç¡®è®¤ ? å·²ç¡®è®¤
                gripper_states_raw, _ = robot_dds.gripper_states() 
                left_gripper_state = gripper_states_raw[0]
                right_gripper_state = gripper_states_raw[1]
                left_6d = quat_to_rotate6d(np.array([left_cartesian["orientation"]["quaternion"]["x"], 
                                            left_cartesian["orientation"]["quaternion"]["y"], 
                                            left_cartesian["orientation"]["quaternion"]["z"], 
                                            left_cartesian["orientation"]["quaternion"]["w"]]))
                
                right_6d = quat_to_rotate6d(np.array([right_cartesian["orientation"]["quaternion"]["x"], 
                                             right_cartesian["orientation"]["quaternion"]["y"], 
                                             right_cartesian["orientation"]["quaternion"]["z"], 
                                             right_cartesian["orientation"]["quaternion"]["w"]]))
                # 16ç»´: left xyz + left 6d + left gripper + right xyz + right 6d + right gripper
                eef_pose_state = np.concatenate([
                    np.array([left_cartesian["position"]["x"], 
                              left_cartesian["position"]["y"], 
                              left_cartesian["position"]["z"]]),
                    left_6d, 
                    np.array([left_gripper_state]),
                    np.array([right_cartesian["position"]["x"], 
                            right_cartesian["position"]["y"], 
                            right_cartesian["position"]["z"]]),
                    right_6d, 
                    np.array([right_gripper_state]),
                ])

                # print(robot_dds.arm_joint_states()[0].shape)
                joint_pose_state = to_flat_array(robot_dds.arm_joint_states()[0])
                # print('suc')
                joint_pose_state = np.concatenate([joint_pose_state, 
                                                   to_flat_array(left_gripper_state), 
                                                   to_flat_array(right_gripper_state)]) # 7+7+1+1=16ç»´

            except (KeyError, IndexError) as e:
                 print(f"âŒ è·å–æœºå™¨äººçŠ¶æ€å¤±è´¥: {e}ã€‚è·³è¿‡æœ¬è½®å¾ªç¯ã€‚")
                 continue
            except Exception as e:
                 print(f"âŒ è·å–æœºå™¨äººçŠ¶æ€æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}ã€‚è·³è¿‡æœ¬è½®å¾ªç¯ã€‚")
                 continue
            
            # è·å–ã€ç¼–ç å¹¶ä¿å­˜å›¾åƒ
            encoded_images = {}
            timestamp = datetime.datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
            for server_name, sdk_name in CAMERA_MAPPING.items():
                raw_img, _ = get_and_encode_image(camera, sdk_name)
                raw_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB) if raw_img is not None else None
                encoded_images[server_name] = raw_img # we use raw image
                if raw_img is not None:
                    print(f"âœ… å·²è·å–å¹¶ç¼–ç å›¾åƒ: {server_name}")
                else:
                    print(f"âš ï¸ æ— æ³•è·å–å›¾åƒ: {server_name}")
                # if raw_img is not None:
                #     log_path = os.path.join(LOG_IMAGE_DIR, f"{server_name}_{timestamp}.png")
                #     cv2.imwrite(log_path, raw_img)
                #     latest_path = f"./{server_name}_latest.png"
                #     cv2.imwrite(latest_path, raw_img)
                #     print(f"[Saved] {latest_path}")
            
            # --- 2.4. è§£æå¹¶æ‰§è¡ŒåŠ¨ä½œ ---
            if "eef" in args.control_mode:
                action = agent.step(encoded_images, eef_pose_state, current_instruction)
                print(f"[Step {count}] with action: {action}")
                if action.shape[0] != 16:
                    print(f"[!] åŠ¨ä½œç»´åº¦ä¸æ­£ç¡® (åº”ä¸º16)ï¼Œè·³è¿‡æ­¤åŠ¨ä½œ: {action}")
                    continue

                left_pose_array, right_pose_array = action[0:8], action[8:16]
                gripper_states = [left_pose_array[7].item(), right_pose_array[7].item()]
                left_pose_dict = { "x": left_pose_array[0].item(), 
                                  "y": left_pose_array[1].item(),
                                  "z": left_pose_array[2].item(),
                                  "qx": left_pose_array[3].item(),
                                  "qy": left_pose_array[4].item(),
                                  "qz": left_pose_array[5].item(),
                                  "qw": left_pose_array[6].item() }
                
                right_pose_dict = { "x": right_pose_array[0].item(), 
                                   "y": right_pose_array[1].item(), 
                                   "z": right_pose_array[2].item(), 
                                   "qx": right_pose_array[3].item(), 
                                   "qy": right_pose_array[4].item(), 
                                   "qz": right_pose_array[5].item(), 
                                   "qw": right_pose_array[6].item()}
                
                robot_controller.set_end_effector_pose_control(
                    lifetime=1.0,
                    control_group=["dual_arm"],
                    right_pose=right_pose_dict,
                    left_pose=left_pose_dict,
                )
                gripper_states = [35 if x < 0.5 else 120 for x in gripper_states] # to angle
                robot_dds.move_gripper(gripper_states)
            elif "joint" in args.control_mode:
                action = agent.step(encoded_images, joint_pose_state, current_instruction)
                
                print(f"[Step {count}] with action: {action}")
                if action.shape[0] != 16:
                    print(f"[!] åŠ¨ä½œç»´åº¦ä¸æ­£ç¡® (åº”ä¸º16)ï¼Œè·³è¿‡æ­¤åŠ¨ä½œ: {action}")
                    continue
                left_joints = action[0:7]
                right_joints = action[7:14]
                gripper_states = [action[-2].item(), action[-1].item()]
                robot_controller.set_joint_position_control( # check this function in A2D SDK
                    lifetime=1.0,
                    joint_group={
                        "left_arm": left_joints.tolist(),
                        "right_arm": right_joints.tolist()
                    }
                )
                gripper_states = [35 if x < 0.1 else 120 for x in gripper_states] # to angle
                robot_dds.move_gripper(gripper_states)
            else:
                print(f"Unsupported control mode: {args.control_mode}")
                break

    except KeyboardInterrupt:
        print("\n[Main] ç”¨æˆ·æ‰‹åŠ¨ä¸­æ–­ç¨‹åºã€‚")
    except Exception as e:
        print(f"\n[Main] âŒ ç¨‹åºæ‰§è¡Œæ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")
    finally:
        # --- 3. å®‰å…¨å…³é—­ ---
        if robot_dds:
            print("\n[Main] é‡ç½®æœºå™¨äººåˆ°å®‰å…¨ä½ç½®...")
            robot_dds.reset()
            # time.sleep(2)
            # robot_dds.shutdown()
        if camera: camera.close()
        print("[Main] ç¨‹åºå·²å®‰å…¨é€€å‡ºã€‚")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="config for agibot client")
    parser.add_argument("--server_ip", type=str, required=True, help="server ip address")
    parser.add_argument("--server_port", type=int, default=8000, help="server port")
    parser.add_argument("--control_mode", type=str, default="abs_eef", choices=["abs_eef", "delta_eef", "abs_joint", "delta_joint"], help="control mode")
    parser.add_argument("--chunk_size", type=int, default=20, help="number of actions to execute per inference")
    parser.add_argument("--close_loop", action="store_true", help="whether to run in closed-loop mode")
    parser.add_argument('--task_id', type=int, default=0, choices=[0,1,2,3,4,5], help='6 different tasks')
    parser.add_argument('--control_freq', type=int, default=50, help='control frequency (Hz)')

    args = parser.parse_args()
    main(args)