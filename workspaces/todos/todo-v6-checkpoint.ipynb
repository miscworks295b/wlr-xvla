{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8bab9470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Florence2ForConditionalGeneration has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "from xvla_wlr.model_legacy import XVLA, XVLAProcessor, Trainer, get_peft_model, Action, Observation\n",
    "\n",
    "model = XVLA.from_pretrained(\"2toINF/X-VLA-SoftFold\")\n",
    "processor = XVLAProcessor.from_pretrained(\"2toINF/X-VLA-SoftFold\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "011e458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import peft\n",
    "\n",
    "model.add_adapter(\n",
    "    peft.LoraConfig(\n",
    "        lora_alpha=16,\n",
    "        r=8,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        modules_to_save=[\n",
    "            \"transformer.soft_prompt_hub\",\n",
    "            \"transformer.action_encoder\",\n",
    "            \"transformer.action_decoder\",\n",
    "        ],\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec2f574d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"samples/checkpoint-adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98511656",
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = XVLA.from_pretrained(\"samples/checkpoint-adapter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf38730c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "aa.load_adapter(\n",
      "    peft_model_id: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    adapter_name: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    revision: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    token: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    device_map: Optional[str] = \u001b[33m'auto'\u001b[39m,\n",
      "    max_memory: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    offload_folder: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    offload_index: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    peft_config: Dict[str, Any] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    adapter_state_dict: Optional[Dict[str, ForwardRef(\u001b[33m'torch.Tensor'\u001b[39m)]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    low_cpu_mem_usage: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    is_trainable: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    adapter_kwargs: Optional[Dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT methods, we\n",
      "invite you to read more about them on PEFT official documentation: https://huggingface.co/docs/peft\n",
      "\n",
      "Requires peft as a backend to load the adapter weights.\n",
      "\n",
      "Args:\n",
      "    peft_model_id (`str`, *optional*):\n",
      "        The identifier of the model to look for on the Hub, or a local path to the saved adapter config file\n",
      "        and adapter weights.\n",
      "    adapter_name (`str`, *optional*):\n",
      "        The adapter name to use. If not set, will use the default adapter.\n",
      "    revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "        The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      "        git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      "        identifier allowed by git.\n",
      "\n",
      "        <Tip>\n",
      "\n",
      "        To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\"`.\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    token (`str`, `optional`):\n",
      "        Whether to use authentication token to load the remote folder. Useful to load private repositories\n",
      "        that are on HuggingFace Hub. You might need to call `huggingface-cli login` and paste your tokens to\n",
      "        cache it.\n",
      "    device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n",
      "        A map that specifies where each submodule should go. It doesn't need to be refined to each\n",
      "        parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n",
      "        same device. If we only pass the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank\n",
      "        like `1`) on which the model will be allocated, the device map will map the entire model to this\n",
      "        device. Passing `device_map = 0` means put the whole model on GPU 0.\n",
      "\n",
      "        To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\n",
      "        more information about each option see [designing a device\n",
      "        map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n",
      "    max_memory (`Dict`, *optional*):\n",
      "        A dictionary device identifier to maximum memory. Will default to the maximum memory available for each\n",
      "        GPU and the available CPU RAM if unset.\n",
      "    offload_folder (`str` or `os.PathLike`, `optional`):\n",
      "        If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n",
      "    offload_index (`int`, `optional`):\n",
      "        `offload_index` argument to be passed to `accelerate.dispatch_model` method.\n",
      "    peft_config (`Dict[str, Any]`, *optional*):\n",
      "        The configuration of the adapter to add, supported adapters are non-prefix tuning and adaption prompts\n",
      "        methods. This argument is used in case users directly pass PEFT state dicts\n",
      "    adapter_state_dict (`Dict[str, torch.Tensor]`, *optional*):\n",
      "        The state dict of the adapter to load. This argument is used in case users directly pass PEFT state\n",
      "        dicts\n",
      "    low_cpu_mem_usage (`bool`, *optional*, defaults to `False`):\n",
      "        Reduce memory usage while loading the PEFT adapter. This should also speed up the loading process.\n",
      "        Requires PEFT version 0.13.0 or higher.\n",
      "    is_trainable (`bool`, *optional*, defaults to `False`):\n",
      "        Whether the adapter should be trainable or not. If `False`, the adapter will be frozen and can only be\n",
      "        used for inference.\n",
      "    adapter_kwargs (`Dict[str, Any]`, *optional*):\n",
      "        Additional keyword arguments passed along to the `from_pretrained` method of the adapter config and\n",
      "        `find_adapter_config_file` method.\n",
      "\u001b[31mFile:\u001b[39m      ~/X-VLA/.conda/lib/python3.11/site-packages/transformers/integrations/peft.py\n",
      "\u001b[31mType:\u001b[39m      method"
     ]
    }
   ],
   "source": [
    "aa.active_adapters()\n",
    "aa.load_adapter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eccfbe09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "aa.load_adapter(\n",
      "    peft_model_id: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    adapter_name: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    revision: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    token: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    device_map: Optional[str] = \u001b[33m'auto'\u001b[39m,\n",
      "    max_memory: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    offload_folder: Optional[str] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    offload_index: Optional[int] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    peft_config: Dict[str, Any] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    adapter_state_dict: Optional[Dict[str, ForwardRef(\u001b[33m'torch.Tensor'\u001b[39m)]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    low_cpu_mem_usage: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    is_trainable: bool = \u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    adapter_kwargs: Optional[Dict[str, Any]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n",
      ") -> \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Load adapter weights from file or remote Hub folder. If you are not familiar with adapters and PEFT methods, we\n",
      "invite you to read more about them on PEFT official documentation: https://huggingface.co/docs/peft\n",
      "\n",
      "Requires peft as a backend to load the adapter weights.\n",
      "\n",
      "Args:\n",
      "    peft_model_id (`str`, *optional*):\n",
      "        The identifier of the model to look for on the Hub, or a local path to the saved adapter config file\n",
      "        and adapter weights.\n",
      "    adapter_name (`str`, *optional*):\n",
      "        The adapter name to use. If not set, will use the default adapter.\n",
      "    revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "        The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      "        git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      "        identifier allowed by git.\n",
      "\n",
      "        <Tip>\n",
      "\n",
      "        To test a pull request you made on the Hub, you can pass `revision=\"refs/pr/<pr_number>\"`.\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    token (`str`, `optional`):\n",
      "        Whether to use authentication token to load the remote folder. Useful to load private repositories\n",
      "        that are on HuggingFace Hub. You might need to call `huggingface-cli login` and paste your tokens to\n",
      "        cache it.\n",
      "    device_map (`str` or `Dict[str, Union[int, str, torch.device]]` or `int` or `torch.device`, *optional*):\n",
      "        A map that specifies where each submodule should go. It doesn't need to be refined to each\n",
      "        parameter/buffer name, once a given module name is inside, every submodule of it will be sent to the\n",
      "        same device. If we only pass the device (*e.g.*, `\"cpu\"`, `\"cuda:1\"`, `\"mps\"`, or a GPU ordinal rank\n",
      "        like `1`) on which the model will be allocated, the device map will map the entire model to this\n",
      "        device. Passing `device_map = 0` means put the whole model on GPU 0.\n",
      "\n",
      "        To have Accelerate compute the most optimized `device_map` automatically, set `device_map=\"auto\"`. For\n",
      "        more information about each option see [designing a device\n",
      "        map](https://hf.co/docs/accelerate/main/en/usage_guides/big_modeling#designing-a-device-map).\n",
      "    max_memory (`Dict`, *optional*):\n",
      "        A dictionary device identifier to maximum memory. Will default to the maximum memory available for each\n",
      "        GPU and the available CPU RAM if unset.\n",
      "    offload_folder (`str` or `os.PathLike`, `optional`):\n",
      "        If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n",
      "    offload_index (`int`, `optional`):\n",
      "        `offload_index` argument to be passed to `accelerate.dispatch_model` method.\n",
      "    peft_config (`Dict[str, Any]`, *optional*):\n",
      "        The configuration of the adapter to add, supported adapters are non-prefix tuning and adaption prompts\n",
      "        methods. This argument is used in case users directly pass PEFT state dicts\n",
      "    adapter_state_dict (`Dict[str, torch.Tensor]`, *optional*):\n",
      "        The state dict of the adapter to load. This argument is used in case users directly pass PEFT state\n",
      "        dicts\n",
      "    low_cpu_mem_usage (`bool`, *optional*, defaults to `False`):\n",
      "        Reduce memory usage while loading the PEFT adapter. This should also speed up the loading process.\n",
      "        Requires PEFT version 0.13.0 or higher.\n",
      "    is_trainable (`bool`, *optional*, defaults to `False`):\n",
      "        Whether the adapter should be trainable or not. If `False`, the adapter will be frozen and can only be\n",
      "        used for inference.\n",
      "    adapter_kwargs (`Dict[str, Any]`, *optional*):\n",
      "        Additional keyword arguments passed along to the `from_pretrained` method of the adapter config and\n",
      "        `find_adapter_config_file` method.\n",
      "\u001b[31mFile:\u001b[39m      ~/X-VLA/.conda/lib/python3.11/site-packages/transformers/integrations/peft.py\n",
      "\u001b[31mType:\u001b[39m      method"
     ]
    }
   ],
   "source": [
    "aa.load_adapter?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eba2413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_assisted_decoding',\n",
       " '_auto_class',\n",
       " '_autoset_attn_implementation',\n",
       " '_backward_compatibility_gradient_checkpointing',\n",
       " '_backward_hooks',\n",
       " '_backward_pre_hooks',\n",
       " '_beam_search',\n",
       " '_beam_search_has_unfinished_sequences',\n",
       " '_buffers',\n",
       " '_build_app',\n",
       " '_cache_dependant_input_preparation',\n",
       " '_cache_dependant_input_preparation_exporting',\n",
       " '_call_impl',\n",
       " '_check_and_enable_flash_attn_2',\n",
       " '_check_and_enable_flex_attn',\n",
       " '_check_and_enable_sdpa',\n",
       " '_compiled_call_impl',\n",
       " '_constrained_beam_search',\n",
       " '_contrastive_search',\n",
       " '_convert_head_mask_to_5d',\n",
       " '_copy_lm_head_original_to_resized',\n",
       " '_create_repo',\n",
       " '_dispatch_accelerate_model',\n",
       " '_dola_decoding',\n",
       " '_expand_inputs_for_generation',\n",
       " '_fix_state_dict_key_on_load',\n",
       " '_fix_state_dict_key_on_save',\n",
       " '_fix_state_dict_keys_on_save',\n",
       " '_flatten_beam_dim',\n",
       " '_forward_hooks',\n",
       " '_forward_hooks_always_called',\n",
       " '_forward_hooks_with_kwargs',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_pre_hooks_with_kwargs',\n",
       " '_from_config',\n",
       " '_gather_beams',\n",
       " '_get_backward_hooks',\n",
       " '_get_backward_pre_hooks',\n",
       " '_get_cache',\n",
       " '_get_candidate_generator',\n",
       " '_get_files_timestamps',\n",
       " '_get_initial_cache_position',\n",
       " '_get_key_renaming_mapping',\n",
       " '_get_layer_device_map_for_cache_init',\n",
       " '_get_logits_processor',\n",
       " '_get_name',\n",
       " '_get_no_split_modules',\n",
       " '_get_resized_embeddings',\n",
       " '_get_resized_lm_head',\n",
       " '_get_running_beams_for_next_iteration',\n",
       " '_get_stopping_criteria',\n",
       " '_get_top_k_continuations',\n",
       " '_group_beam_search',\n",
       " '_has_unfinished_sequences',\n",
       " '_hf_peft_config_loaded',\n",
       " '_hf_peft_config_loaded',\n",
       " '_hook_rss_memory_post_forward',\n",
       " '_hook_rss_memory_pre_forward',\n",
       " '_init_added_embeddings_weights_with_mean',\n",
       " '_init_added_lm_head_bias_with_mean',\n",
       " '_init_added_lm_head_weights_with_mean',\n",
       " '_init_weights',\n",
       " '_initialize_missing_keys',\n",
       " '_initialize_weights',\n",
       " '_is_full_backward_hook',\n",
       " '_is_hf_initialized',\n",
       " '_is_stateful',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keep_in_fp32_modules',\n",
       " '_keys_to_ignore_on_load_missing',\n",
       " '_keys_to_ignore_on_load_unexpected',\n",
       " '_keys_to_ignore_on_save',\n",
       " '_load_from_flax',\n",
       " '_load_from_state_dict',\n",
       " '_load_from_tf',\n",
       " '_load_pretrained_model',\n",
       " '_load_state_dict_post_hooks',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_maybe_initialize_input_ids_for_generation',\n",
       " '_maybe_warn_non_full_backward_hook',\n",
       " '_merge_criteria_processor_list',\n",
       " '_modules',\n",
       " '_move_missing_keys_from_meta_to_cpu',\n",
       " '_named_members',\n",
       " '_no_split_modules',\n",
       " '_no_split_modules',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_pp_plan',\n",
       " '_prefill_chunking',\n",
       " '_prepare_attention_mask_for_generation',\n",
       " '_prepare_cache_for_generation',\n",
       " '_prepare_decoder_input_ids_for_generation',\n",
       " '_prepare_encoder_decoder_kwargs_for_generation',\n",
       " '_prepare_generated_length',\n",
       " '_prepare_generation_config',\n",
       " '_prepare_model_inputs',\n",
       " '_prepare_special_tokens',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_reorder_cache',\n",
       " '_replicate_for_data_parallel',\n",
       " '_resize_token_embeddings',\n",
       " '_sample',\n",
       " '_save_to_state_dict',\n",
       " '_set_default_torch_dtype',\n",
       " '_set_gradient_checkpointing',\n",
       " '_skip_keys_device_placement',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_state_dict_pre_hooks',\n",
       " '_supports_attention_backend',\n",
       " '_supports_cache_class',\n",
       " '_supports_default_dynamic_cache',\n",
       " '_supports_flash_attn_2',\n",
       " '_supports_flex_attn',\n",
       " '_supports_logits_to_keep',\n",
       " '_supports_quantized_cache',\n",
       " '_supports_sdpa',\n",
       " '_supports_static_cache',\n",
       " '_temporary_reorder_cache',\n",
       " '_tie_encoder_decoder_weights',\n",
       " '_tie_or_clone_weights',\n",
       " '_tied_weights_keys',\n",
       " '_tp_plan',\n",
       " '_unflatten_beam_dim',\n",
       " '_update_finished_beams',\n",
       " '_update_model_kwargs_for_generation',\n",
       " '_upload_modified_files',\n",
       " '_validate_assistant',\n",
       " '_validate_generated_length',\n",
       " '_validate_model_class',\n",
       " '_validate_model_kwargs',\n",
       " '_version',\n",
       " '_wrapped_call_impl',\n",
       " 'action_mode',\n",
       " 'action_space',\n",
       " 'active_adapter',\n",
       " 'active_adapters',\n",
       " 'add_adapter',\n",
       " 'add_memory_hooks',\n",
       " 'add_model_tags',\n",
       " 'add_module',\n",
       " 'app',\n",
       " 'apply',\n",
       " 'base_model',\n",
       " 'base_model_prefix',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'call_super_init',\n",
       " 'can_generate',\n",
       " 'children',\n",
       " 'compile',\n",
       " 'compute_transition_scores',\n",
       " 'config',\n",
       " 'config_class',\n",
       " 'cpu',\n",
       " 'create_extended_attention_mask_for_decoder',\n",
       " 'cuda',\n",
       " 'delete_adapter',\n",
       " 'dequantize',\n",
       " 'device',\n",
       " 'disable_adapters',\n",
       " 'disable_input_require_grads',\n",
       " 'double',\n",
       " 'dtype',\n",
       " 'dummy_inputs',\n",
       " 'dump_patches',\n",
       " 'enable_adapters',\n",
       " 'enable_input_require_grads',\n",
       " 'estimate_tokens',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'floating_point_ops',\n",
       " 'forward',\n",
       " 'forward_vlm',\n",
       " 'framework',\n",
       " 'from_pretrained',\n",
       " 'generate',\n",
       " 'generate_actions',\n",
       " 'generation_config',\n",
       " 'get_adapter_state_dict',\n",
       " 'get_buffer',\n",
       " 'get_compiled_call',\n",
       " 'get_extended_attention_mask',\n",
       " 'get_extra_state',\n",
       " 'get_head_mask',\n",
       " 'get_init_context',\n",
       " 'get_input_embeddings',\n",
       " 'get_memory_footprint',\n",
       " 'get_output_embeddings',\n",
       " 'get_parameter',\n",
       " 'get_parameter_or_buffer',\n",
       " 'get_position_embeddings',\n",
       " 'get_submodule',\n",
       " 'gradient_checkpointing_disable',\n",
       " 'gradient_checkpointing_enable',\n",
       " 'half',\n",
       " 'heal_tokens',\n",
       " 'init_weights',\n",
       " 'invert_attention_mask',\n",
       " 'ipu',\n",
       " 'is_backend_compatible',\n",
       " 'is_gradient_checkpointing',\n",
       " 'is_parallelizable',\n",
       " 'load_adapter',\n",
       " 'load_state_dict',\n",
       " 'loss_function',\n",
       " 'loss_type',\n",
       " 'main_input_name',\n",
       " 'model_tags',\n",
       " 'modules',\n",
       " 'mtia',\n",
       " 'name_or_path',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'num_actions',\n",
       " 'num_parameters',\n",
       " 'parameters',\n",
       " 'peft_config',\n",
       " 'post_init',\n",
       " 'prepare_inputs_for_generation',\n",
       " 'prune_heads',\n",
       " 'push_to_hub',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_for_auto_class',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_full_backward_hook',\n",
       " 'register_full_backward_pre_hook',\n",
       " 'register_load_state_dict_post_hook',\n",
       " 'register_load_state_dict_pre_hook',\n",
       " 'register_module',\n",
       " 'register_parameter',\n",
       " 'register_state_dict_post_hook',\n",
       " 'register_state_dict_pre_hook',\n",
       " 'requires_grad_',\n",
       " 'reset_memory_hooks_state',\n",
       " 'resize_position_embeddings',\n",
       " 'resize_token_embeddings',\n",
       " 'retrieve_modules_from_names',\n",
       " 'reverse_bettertransformer',\n",
       " 'run',\n",
       " 'save_pretrained',\n",
       " 'set_adapter',\n",
       " 'set_extra_state',\n",
       " 'set_input_embeddings',\n",
       " 'set_submodule',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'supports_gradient_checkpointing',\n",
       " 'supports_pp_plan',\n",
       " 'supports_tp_plan',\n",
       " 'tie_weights',\n",
       " 'to',\n",
       " 'to_bettertransformer',\n",
       " 'to_empty',\n",
       " 'train',\n",
       " 'training',\n",
       " 'transformer',\n",
       " 'type',\n",
       " 'use_proprio',\n",
       " 'vlm',\n",
       " 'warn_if_padding_and_no_attention_mask',\n",
       " 'warnings_issued',\n",
       " 'xpu',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c1e8c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_peft_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6420b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"samples/checkpoint-peft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "34b62758",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.save_pretrained(\"samples/checkpoint-peft-shallow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "05b83fac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInit signature:\u001b[39m XVLA(config: \u001b[33m'XVLAConfig'\u001b[39m, *args, **kwargs)\n",
      "\u001b[31mDocstring:\u001b[39m     \n",
      "XVLA: HuggingFace-compatible Vision-Language-Action policy.\n",
      "\n",
      "Components:\n",
      "  â€¢ Florence2 encoder-only backbone (vision-language)\n",
      "  â€¢ SoftPromptedTransformer (temporal/action head)\n",
      "  â€¢ Action space (pre/post-processing + loss)\n",
      "\u001b[31mInit docstring:\u001b[39m Initialize internal Module state, shared by both nn.Module and ScriptModule.\n",
      "\u001b[31mFile:\u001b[39m           ~/X-VLA/packages/xvla_wlr/xvla/models/modeling_xvla.py\n",
      "\u001b[31mType:\u001b[39m           type\n",
      "\u001b[31mSubclasses:\u001b[39m     "
     ]
    }
   ],
   "source": [
    "XVLA?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c1099",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75e5ff85",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory samples/checkpoint-peft-shallow.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mXVLA\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msamples/checkpoint-peft-shallow\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/X-VLA/.conda/lib/python3.11/site-packages/transformers/modeling_utils.py:279\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    277\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    281\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/X-VLA/.conda/lib/python3.11/site-packages/transformers/modeling_utils.py:4260\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4250\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4251\u001b[39m     gguf_file\n\u001b[32m   4252\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4253\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4254\u001b[39m ):\n\u001b[32m   4255\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4256\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4257\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4260\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4262\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4264\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4267\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4273\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4275\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4276\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4278\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4279\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/X-VLA/.conda/lib/python3.11/site-packages/transformers/modeling_utils.py:952\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[39m\n\u001b[32m    947\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    948\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m found in directory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    949\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    950\u001b[39m         )\n\u001b[32m    951\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m952\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m    953\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mError no file named \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(SAFE_WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    954\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF2_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mTF_WEIGHTS_NAME\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[33m'\u001b[39m\u001b[33m.index\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m or \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mFLAX_WEIGHTS_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m found in directory\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    955\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    956\u001b[39m         )\n\u001b[32m    957\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m os.path.isfile(os.path.join(subfolder, pretrained_model_name_or_path)):\n\u001b[32m    958\u001b[39m     archive_file = pretrained_model_name_or_path\n",
      "\u001b[31mOSError\u001b[39m: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory samples/checkpoint-peft-shallow."
     ]
    }
   ],
   "source": [
    "XVLA.from_pretrained(pretrained_model_name_or_path=\"samples/checkpoint-peft-shallow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4d447fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel, AutoPeftModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3eff27a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModel(\n",
       "  (base_model): LoraModel(\n",
       "    (model): XVLA(\n",
       "      (action_space): EE6DActionSpace(\n",
       "        (mse): MSELoss()\n",
       "        (bce): BCEWithLogitsLoss()\n",
       "      )\n",
       "      (vlm): Florence2ForConditionalGeneration(\n",
       "        (vision_tower): DaViT(\n",
       "          (convs): ModuleList(\n",
       "            (0): ConvEmbed(\n",
       "              (proj): Conv2d(3, 256, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (1): ConvEmbed(\n",
       "              (proj): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (2): ConvEmbed(\n",
       "              (proj): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "            (3): ConvEmbed(\n",
       "              (proj): Conv2d(1024, 2048, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "              (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (blocks): ModuleList(\n",
       "            (0): MySequential(\n",
       "              (0): MySequential(\n",
       "                (spatial_block): SpatialBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                    )\n",
       "                  )\n",
       "                  (window_attn): PreNorm(\n",
       "                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): WindowAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=256, out_features=768, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=256, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=256, out_features=256, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=256, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (softmax): Softmax(dim=-1)\n",
       "                    )\n",
       "                    (drop_path): Identity()\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=256, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): Identity()\n",
       "                  )\n",
       "                )\n",
       "                (channel_block): ChannelBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                    )\n",
       "                  )\n",
       "                  (channel_attn): PreNorm(\n",
       "                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): ChannelAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=256, out_features=768, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=256, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=768, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=256, out_features=256, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=256, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.004)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=256, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.004)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (1): MySequential(\n",
       "              (0): MySequential(\n",
       "                (spatial_block): SpatialBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                    )\n",
       "                  )\n",
       "                  (window_attn): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): WindowAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1536, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (softmax): Softmax(dim=-1)\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.009)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.009)\n",
       "                  )\n",
       "                )\n",
       "                (channel_block): ChannelBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                    )\n",
       "                  )\n",
       "                  (channel_attn): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): ChannelAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=512, out_features=1536, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1536, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=512, out_features=512, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.013)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=512, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=512, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.013)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (2): MySequential(\n",
       "              (0): MySequential(\n",
       "                (spatial_block): SpatialBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (window_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): WindowAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (softmax): Softmax(dim=-1)\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.017)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.017)\n",
       "                  )\n",
       "                )\n",
       "                (channel_block): ChannelBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (channel_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): ChannelAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.022)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.022)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (1): MySequential(\n",
       "                (spatial_block): SpatialBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (window_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): WindowAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (softmax): Softmax(dim=-1)\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.026)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.026)\n",
       "                  )\n",
       "                )\n",
       "                (channel_block): ChannelBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (channel_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): ChannelAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.030)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.030)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (2): MySequential(\n",
       "                (spatial_block): SpatialBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (window_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): WindowAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (softmax): Softmax(dim=-1)\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.035)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.035)\n",
       "                  )\n",
       "                )\n",
       "                (channel_block): ChannelBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (channel_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): ChannelAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.039)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.039)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (3): MySequential(\n",
       "                (spatial_block): SpatialBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (window_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): WindowAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (softmax): Softmax(dim=-1)\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.043)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.043)\n",
       "                  )\n",
       "                )\n",
       "                (channel_block): ChannelBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (channel_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): ChannelAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.048)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.048)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (4): MySequential(\n",
       "                (spatial_block): SpatialBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (window_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): WindowAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (softmax): Softmax(dim=-1)\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.052)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.052)\n",
       "                  )\n",
       "                )\n",
       "                (channel_block): ChannelBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (channel_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): ChannelAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.057)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.057)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (5): MySequential(\n",
       "                (spatial_block): SpatialBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (window_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): WindowAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (softmax): Softmax(dim=-1)\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.061)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.061)\n",
       "                  )\n",
       "                )\n",
       "                (channel_block): ChannelBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (channel_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): ChannelAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.065)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.065)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (6): MySequential(\n",
       "                (spatial_block): SpatialBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (window_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): WindowAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (softmax): Softmax(dim=-1)\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.070)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.070)\n",
       "                  )\n",
       "                )\n",
       "                (channel_block): ChannelBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (channel_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): ChannelAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.074)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.074)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (7): MySequential(\n",
       "                (spatial_block): SpatialBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (window_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): WindowAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (softmax): Softmax(dim=-1)\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.078)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.078)\n",
       "                  )\n",
       "                )\n",
       "                (channel_block): ChannelBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (channel_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): ChannelAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.083)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.083)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (8): MySequential(\n",
       "                (spatial_block): SpatialBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (window_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): WindowAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (softmax): Softmax(dim=-1)\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.087)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.087)\n",
       "                  )\n",
       "                )\n",
       "                (channel_block): ChannelBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (channel_attn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): ChannelAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.091)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.091)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "            (3): MySequential(\n",
       "              (0): MySequential(\n",
       "                (spatial_block): SpatialBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                    )\n",
       "                  )\n",
       "                  (window_attn): PreNorm(\n",
       "                    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): WindowAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=6144, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (softmax): Softmax(dim=-1)\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.096)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.096)\n",
       "                  )\n",
       "                )\n",
       "                (channel_block): ChannelBlock(\n",
       "                  (conv1): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                    )\n",
       "                  )\n",
       "                  (channel_attn): PreNorm(\n",
       "                    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): ChannelAttention(\n",
       "                      (qkv): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=2048, out_features=6144, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=6144, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                      (proj): lora.Linear(\n",
       "                        (base_layer): Linear(in_features=2048, out_features=2048, bias=True)\n",
       "                        (lora_dropout): ModuleDict(\n",
       "                          (default): Identity()\n",
       "                        )\n",
       "                        (lora_A): ModuleDict(\n",
       "                          (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                        )\n",
       "                        (lora_B): ModuleDict(\n",
       "                          (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                        )\n",
       "                        (lora_embedding_A): ParameterDict()\n",
       "                        (lora_embedding_B): ParameterDict()\n",
       "                        (lora_magnitude_vector): ModuleDict()\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.100)\n",
       "                  )\n",
       "                  (conv2): PreNorm(\n",
       "                    (fn): DepthWiseConv2d(\n",
       "                      (dw): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048)\n",
       "                    )\n",
       "                  )\n",
       "                  (ffn): PreNorm(\n",
       "                    (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)\n",
       "                    (fn): Mlp(\n",
       "                      (net): Sequential(\n",
       "                        (fc1): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=2048, out_features=8192, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                        (act): GELU(approximate='none')\n",
       "                        (fc2): lora.Linear(\n",
       "                          (base_layer): Linear(in_features=8192, out_features=2048, bias=True)\n",
       "                          (lora_dropout): ModuleDict(\n",
       "                            (default): Identity()\n",
       "                          )\n",
       "                          (lora_A): ModuleDict(\n",
       "                            (default): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                          )\n",
       "                          (lora_B): ModuleDict(\n",
       "                            (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                          )\n",
       "                          (lora_embedding_A): ParameterDict()\n",
       "                          (lora_embedding_B): ParameterDict()\n",
       "                          (lora_magnitude_vector): ModuleDict()\n",
       "                        )\n",
       "                      )\n",
       "                    )\n",
       "                    (drop_path): DropPath(drop_prob=0.100)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (avgpool): AdaptiveAvgPool1d(output_size=1)\n",
       "        )\n",
       "        (image_proj_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (image_pos_embed): LearnedAbsolutePositionEmbedding2D(\n",
       "          (row_embeddings): Embedding(50, 1024)\n",
       "          (column_embeddings): Embedding(50, 1024)\n",
       "        )\n",
       "        (visual_temporal_embed): PositionalEmbeddingCosine1D()\n",
       "        (language_model): Florence2LanguageForConditionalGeneration(\n",
       "          (model): Florence2LanguageModel(\n",
       "            (shared): Embedding(51289, 1024, padding_idx=1)\n",
       "            (encoder): Florence2Encoder(\n",
       "              (embed_tokens): Florence2ScaledWordEmbedding(51289, 1024, padding_idx=1)\n",
       "              (embed_positions): Florence2LearnedPositionalEmbedding(4098, 1024)\n",
       "              (layers): ModuleList(\n",
       "                (0-11): 12 x Florence2EncoderLayer(\n",
       "                  (self_attn): Florence2SdpaAttention(\n",
       "                    (k_proj): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (v_proj): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (q_proj): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                    (out_proj): lora.Linear(\n",
       "                      (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                      (lora_dropout): ModuleDict(\n",
       "                        (default): Identity()\n",
       "                      )\n",
       "                      (lora_A): ModuleDict(\n",
       "                        (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                      )\n",
       "                      (lora_B): ModuleDict(\n",
       "                        (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                      )\n",
       "                      (lora_embedding_A): ParameterDict()\n",
       "                      (lora_embedding_B): ParameterDict()\n",
       "                      (lora_magnitude_vector): ModuleDict()\n",
       "                    )\n",
       "                  )\n",
       "                  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                  (activation_fn): GELUActivation()\n",
       "                  (fc1): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (fc2): lora.Linear(\n",
       "                    (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Identity()\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "                )\n",
       "              )\n",
       "              (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (transformer): SoftPromptedTransformer(\n",
       "        (blocks): ModuleList(\n",
       "          (0-23): 24 x TransformerBlock(\n",
       "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (attn): Attention(\n",
       "              (qkv): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=3072, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=3072, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (q_norm): Identity()\n",
       "              (k_norm): Identity()\n",
       "              (attn_drop): Dropout(p=0.1, inplace=False)\n",
       "              (proj): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (mlp): Mlp(\n",
       "              (fc1): lora.Linear(\n",
       "                (base_layer): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act): GELU(approximate='tanh')\n",
       "              (drop1): Dropout(p=0.1, inplace=False)\n",
       "              (norm): Identity()\n",
       "              (fc2): lora.Linear(\n",
       "                (base_layer): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (drop2): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (vlm_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Identity()\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (aux_visual_proj): lora.Linear(\n",
       "          (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          (lora_dropout): ModuleDict(\n",
       "            (default): Identity()\n",
       "          )\n",
       "          (lora_A): ModuleDict(\n",
       "            (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "          )\n",
       "          (lora_B): ModuleDict(\n",
       "            (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "          )\n",
       "          (lora_embedding_A): ParameterDict()\n",
       "          (lora_embedding_B): ParameterDict()\n",
       "          (lora_magnitude_vector): ModuleDict()\n",
       "        )\n",
       "        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (action_encoder): ModulesToSaveWrapper(\n",
       "          (original_module): DomainAwareLinear(\n",
       "            (fc): Embedding(30, 73728)\n",
       "            (bias): Embedding(30, 1024)\n",
       "          )\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (default): DomainAwareLinear(\n",
       "              (fc): Embedding(30, 73728)\n",
       "              (bias): Embedding(30, 1024)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (action_decoder): ModulesToSaveWrapper(\n",
       "          (original_module): DomainAwareLinear(\n",
       "            (fc): Embedding(30, 20480)\n",
       "            (bias): Embedding(30, 20)\n",
       "          )\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (default): DomainAwareLinear(\n",
       "              (fc): Embedding(30, 20480)\n",
       "              (bias): Embedding(30, 20)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (soft_prompt_hub): ModulesToSaveWrapper(\n",
       "          (original_module): Embedding(30, 32768)\n",
       "          (modules_to_save): ModuleDict(\n",
       "            (default): Embedding(30, 32768)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PeftModel.from_pretrained(model, \"samples/checkpoint-peft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6c3500d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "todo_model: PeftModel = AutoPeftModel.from_pretrained(\"samples/checkpoint-peft\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af5263b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "todo_model_ = todo_model.get_base_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae22c0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "754fe943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "todo_model_.generate_actions(\n",
      "    input_ids: \u001b[33m'torch.LongTensor'\u001b[39m,\n",
      "    image_input: \u001b[33m'torch.FloatTensor'\u001b[39m,\n",
      "    image_mask: \u001b[33m'torch.Tensor'\u001b[39m,\n",
      "    domain_id: \u001b[33m'torch.LongTensor'\u001b[39m,\n",
      "    proprio: \u001b[33m'torch.Tensor'\u001b[39m,\n",
      "    steps: \u001b[33m'int'\u001b[39m = \u001b[32m10\u001b[39m,\n",
      ") -> \u001b[33m'torch.Tensor'\u001b[39m\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Iterative denoising (linear schedule).\n",
      "Applies action_space.postprocess at the end (e.g., sigmoid on gripper).\n",
      "\u001b[31mFile:\u001b[39m      ~/X-VLA/packages/xvla_wlr/xvla/models/modeling_xvla.py\n",
      "\u001b[31mType:\u001b[39m      method"
     ]
    }
   ],
   "source": [
    "todo_model_.generate_actions?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
